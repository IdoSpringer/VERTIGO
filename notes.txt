
# keep up the good work :)

# todo 2.7.20 - 7.7.20
# order directories                                                         V
# check that current samples have cd4/cd8                                   V
# new mcpas data and samples                                                V
# update code to inclute cd4 cd8 in train                                   V
# MAJOR bugs fix (all features were used as default)                        V
# train 2*2*8 models (we have good hyperparams)                             V
# let's use whole new mcpas
# implement all tests in evaluations                                        V
# check that the tests work well on a single model                          V (fix spb, mps later)
# start comparing to ergo                                                   V (we need tpp-i of about 0.85)
# why tpp-i is lower? less data? sampling?                                  V seems like lack of data
# when we take all features we have high tpp-iii. why? do we overfit?       V (it's ok, we lear the mhc)
# we might overfit... the peptide is new,
# but maybe we already learned its mhc...                                   V legitimate
# WHY WHEN TRAINING OVER ALL MCPAS WE HAVE AUC OF 0.5?                      V
# find this bug!                                                            V
# (it seems more like running cmd intersections rather than actual code bug)
# on full mcpas we get tpp-i of about 0.81 instead of 0.85, seems ok
(so this is a data issue when we train only human mcpas,
maybe just use full database)
# check that train and test are truly separate in evaluation
(numbers seem too high? 0.93)
# according to the features used                                            V (working but seems useless)
# auto args/checkpoint loading                                              V
# 'cross validation' datasets + runs                                        - Not yet
# start tables and plots                                                    V
# Why do we have same samples in train and test but with the opposite sign?
what can cause that? it is really strange                                   -
# how can TPP-III be greater than TPP-I?                                    -
# TPP table runs                                                            V not completed
# do something else meantime...
# spb (will also be slow) comparison to ergo table                          V
# spb bugs (loading + empty batch)                                          V
# BREAKING NEWS: when we compute SPB we do not have to filter test pairs!!!
# perhaps also in TPP-II and TPP-III ?
# check strange results (bugs + more runs?)                                 -
# model pipeline figure                                                     -
# sieving figure?                                                           -
# length distribution
# TCR logo (alpha + beta)
# peptide logo for CD4 vs CD8
# contribution of each component
# later - contribution of positions in alpha/beta
# standard peptides - barplot of cumulative accuracy of each component
# (run the same on all frequent peptides and cluster peptides based on
contribution. stats on clusters)
# much later - yellow fever (filter tcr in test)
# comparision to dash/tcrgp on different components




# how many models to we need to train?
# mcpas, vdjdb * 2
# ae, lstm * 2

# only beta                                         running
# with alpha (only)                                 running
# with vj (only)                                    running
# with mhc (only)                                   running
# with t type (only)                                running
# with alpha, with vj                               running
# with alpha, with vj, with mhc                     running
# with alpha, with vj, with mhc, with t type        running
# * 8

# overall 2*2*8 = 32 models !   :)


nohup python Trainer.py

1 0 mcpas AE                        R
1 1 mcpas LSTM                      R
1 2 vdjdb AE                        R
1 3 vdjdb LSTM                      R

1 4 mcpas AE --use_alpha            R
1 5 mcpas LSTM --use_alpha          R
1 0 vdjdb AE --use_alpha            R
1 1 vdjdb LSTM --use_alpha          R

1 2 mcpas AE --use_vj               R
1 3 mcpas LSTM --use_vj             R
1 4 vdjdb AE --use_vj               R
1 5 vdjdb LSTM --use_vj             R

1 0 mcpas AE --use_mhc              R
1 1 mcpas LSTM --use_mhc            R
1 2 vdjdb AE --use_mhc              R
1 3 vdjdb LSTM --use_mhc            R

1 4 mcpas AE --use_t_type           R
1 5 mcpas LSTM --use_t_type         R
1 0 vdjdb AE --use_t_type           R
1 1 vdjdb LSTM --use_t_type         R

1 2 mcpas AE --use_alpha --use_vj   R
1 3 mcpas LSTM --use_alpha --use_vj R
1 4 vdjdb AE --use_alpha --use_vj   R
1 5 vdjdb LSTM --use_alpha --use_vj R

1 0 mcpas AE --use_alpha --use_vj --use_mhc     R
1 1 mcpas LSTM --use_alpha --use_vj --use_mhc   R
1 2 vdjdb AE --use_alpha --use_vj --use_mhc     R
1 3 vdjdb LSTM --use_alpha --use_vj --use_mhc   R

1 0 mcpas AE --use_alpha --use_vj --use_mhc --use_t_type    R
1 1 mcpas LSTM --use_alpha --use_vj --use_mhc --use_t_type  R
1 2 vdjdb AE --use_alpha --use_vj --use_mhc --use_t_type    R
1 3 vdjdb LSTM --use_alpha --use_vj --use_mhc --use_t_type  R



# *5 for paper = 160 :/ (but *3 or even *1 is also fine)

# lets have flags
# order of flags: iteration, dataset, model, alpha, vj, mhc, t type
# datasets = h for human mcpas, m for mcpas ,v for vdjdb, x for other
# models = e for ae, l for lstm
# alpha = a
# vj = j
# mhc = h
# t type = t

# e.g
# 1maaj = mcpas ae with alpha and mhc
# 2vlt = vdjdb lstm with t cell type

# how to evaluate?
# should be easy, just choose the right model
# can we evaluate with less trained models?
# I think we can't
